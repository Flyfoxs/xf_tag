{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sample.csv',\n",
       " 'apptype_id_name.txt',\n",
       " 'app_desc.dat',\n",
       " 'submit.csv',\n",
       " '.ipynb_checkpoints',\n",
       " 'apptype_train.dat']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入相关库\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import *\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from gensim.models import FastText, Word2Vec\n",
    "import re\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import *\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import keras.backend as K\n",
    "from keras.optimizers import *\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import gc\n",
    "import logging\n",
    "import gensim\n",
    "import jieba\n",
    "tqdm.pandas()\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# 显卡使用（如没显卡需要注释掉）\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "np.random.seed(1024)\n",
    "rn.seed(1024)\n",
    "tf.set_random_seed(1024)\n",
    "path=\"data/\"\n",
    "os.listdir(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "with open(path+ \"app_desc.dat\", encoding='utf-8')as f:\n",
    "    for line in f.readlines():\n",
    "        data.append(line.strip(\"\\r\\n\").split(\"\\t\"))\n",
    "app_desc = pd.DataFrame(data,columns=['id', 'content'])\n",
    "\n",
    "apptype_id_name = pd.read_table(path + 'apptype_id_name.txt', header=None)\n",
    "apptype_id_name.columns = ['appid', 'appshow']\n",
    "sample = pd.read_csv(path + 'sample.csv')\n",
    "f = open(path + 'apptype_train.dat', encoding='utf-8')\n",
    "apptype_train = pd.DataFrame()\n",
    "a1 = []\n",
    "a2 = []\n",
    "a3 = []\n",
    "for i in f.read().split('\\n'):\n",
    "    if len(i.split('\\t')[1].split('|')) == 1: \n",
    "        a1.append(i.split('\\t')[0])\n",
    "        a2.append(i.split('\\t')[1])\n",
    "        a3.append(i.split('\\t')[2])\n",
    "    else:\n",
    "        a1.append(i.split('\\t')[0])\n",
    "        a3.append(i.split('\\t')[2])\n",
    "        a2.append(i.split('\\t')[1].split('|')[0])\n",
    "        a1.append(i.split('\\t')[0])\n",
    "        a3.append(i.split('\\t')[2])\n",
    "        a2.append(i.split('\\t')[1].split('|')[1])\n",
    "apptype_train['id'] = a1\n",
    "apptype_train['content'] = a3\n",
    "apptype_train['label'] = a2\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要用到的函数\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class Lookahead(object):\n",
    "    \"\"\"Add the [Lookahead Optimizer](https://arxiv.org/abs/1907.08610) functionality for [keras](https://keras.io/).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=5, alpha=0.5):\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.count = 0\n",
    "\n",
    "    def inject(self, model):\n",
    "        \"\"\"Inject the Lookahead algorithm for the given model.\n",
    "        The following code is modified from keras's _make_train_function method.\n",
    "        See: https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L497\n",
    "        \"\"\"\n",
    "        if not hasattr(model, 'train_function'):\n",
    "            raise RuntimeError('You must compile your model before using it.')\n",
    "\n",
    "        model._check_trainable_weights_consistency()\n",
    "\n",
    "        if model.train_function is None:\n",
    "            inputs = (model._feed_inputs +\n",
    "                      model._feed_targets +\n",
    "                      model._feed_sample_weights)\n",
    "            if model._uses_dynamic_learning_phase():\n",
    "                inputs += [K.learning_phase()]\n",
    "            fast_params = model._collected_trainable_weights\n",
    "\n",
    "            with K.name_scope('training'):\n",
    "                with K.name_scope(model.optimizer.__class__.__name__):\n",
    "                    training_updates = model.optimizer.get_updates(\n",
    "                        params=fast_params,\n",
    "                        loss=model.total_loss)\n",
    "                    slow_params = [K.variable(p) for p in fast_params]\n",
    "                fast_updates = (model.updates +\n",
    "                                training_updates +\n",
    "                                model.metrics_updates)\n",
    "\n",
    "                slow_updates, copy_updates = [], []\n",
    "                for p, q in zip(fast_params, slow_params):\n",
    "                    slow_updates.append(K.update(q, q + self.alpha * (p - q)))\n",
    "                    copy_updates.append(K.update(p, q))\n",
    "\n",
    "                # Gets loss and metrics. Updates weights at each call.\n",
    "                fast_train_function = K.function(\n",
    "                    inputs,\n",
    "                    [model.total_loss] + model.metrics_tensors,\n",
    "                    updates=fast_updates,\n",
    "                    name='fast_train_function',\n",
    "                    **model._function_kwargs)\n",
    "\n",
    "                def F(inputs):\n",
    "                    self.count += 1\n",
    "                    R = fast_train_function(inputs)\n",
    "                    if self.count % self.k == 0:\n",
    "                        K.batch_get_value(slow_updates)\n",
    "                        K.batch_get_value(copy_updates)\n",
    "                    return R\n",
    "                \n",
    "                model.train_function = F\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1/4)\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        super(AdamW, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            # decoupled weight decay (2/4)\n",
    "            self.wd = K.variable(weight_decay, name='weight_decay')\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        wd = self.wd  # decoupled weight decay (3/4)\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            # decoupled weight decay (4/4)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - lr * wd * p\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.wd)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/131133 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.761 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      " 70%|██████▉   | 91679/131133 [01:50<00:46, 849.91it/s]"
     ]
    }
   ],
   "source": [
    "### 读入数据(分词)\n",
    "data = pd.concat([apptype_train, app_desc], axis=0, sort=False)\n",
    "data['word_cut'] = data['content'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "data['char_cut'] = data['content'].progress_apply(lambda row:' '.join(list(row)))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_len'] = data['word_cut'].apply(lambda row:len(str(row).split(' ')))\n",
    "data['char_len'] = data['char_cut'].apply(lambda row:len(str(row).split(' ')))\n",
    "print('word长度:',np.percentile(data['word_len'], 95))\n",
    "print('char长度:',np.percentile(data['char_len'], 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer 序列化文本\n",
    "def set_tokenizer(docs, split_char=' ', max_len=100):\n",
    "    '''\n",
    "    输入\n",
    "    docs:文本列表\n",
    "    split_char:按什么字符切割\n",
    "    max_len:截取的最大长度\n",
    "    \n",
    "    输出\n",
    "    X:序列化后的数据\n",
    "    word_index:文本和数字对应的索引\n",
    "    '''\n",
    "    tokenizer = Tokenizer(lower=False, char_level=False, split=split_char)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    X = tokenizer.texts_to_sequences(docs)\n",
    "    maxlen = max_len\n",
    "    X = pad_sequences(X, maxlen=maxlen, value=0)\n",
    "    word_index=tokenizer.word_index\n",
    "    return X, word_index\n",
    "\n",
    "### 做embedding 这里采用word2vec 可以换成其他例如（glove词向量）\n",
    "def trian_save_word2vec(docs, embed_size=300, save_name='w2v.txt', split_char=' '):\n",
    "    '''\n",
    "    输入\n",
    "    docs:输入的文本列表\n",
    "    embed_size:embed长度\n",
    "    save_name:保存的word2vec位置\n",
    "    \n",
    "    输出\n",
    "    w2v:返回的模型\n",
    "    '''\n",
    "    input_docs = []\n",
    "    for i in docs:\n",
    "        input_docs.append(i.split(split_char))\n",
    "    logging.basicConfig(\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO)\n",
    "    w2v = Word2Vec(input_docs, size=embed_size, sg=1, window=8, seed=1017, workers=24, min_count=1, iter=10)\n",
    "    w2v.save(save_name)\n",
    "    print(\"w2v model done\")\n",
    "    return w2v\n",
    "\n",
    "# 得到embedding矩阵\n",
    "def get_embedding_matrix(word_index, embed_size=300, Emed_path=\"w2v_300.txt\"):\n",
    "    embeddings_index = Word2Vec.load(Emed_path)\n",
    "    nb_words = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    count = 0\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = np.zeros(embed_size)\n",
    "            count += 1\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector  \n",
    "    print(\"null cnt\",count)\n",
    "    return embedding_matrix\n",
    "\n",
    "# 得到fasttext矩阵\n",
    "def load_fasttext(word_index, path):  \n",
    "    count=0\n",
    "    null_list=[]\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(path, encoding='utf-8') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words =  len(word_index)+1\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            null_list.append(word)\n",
    "            count+=1\n",
    "    print(\"null cnt:\",count)\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_embedding_matrix_txt(word_index,embed_size=200,Emed_path=\"w2v_300.txt\"):\n",
    "    embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        Emed_path, binary=False)\n",
    "    nb_words = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    count = 0\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = np.zeros(embed_size)\n",
    "            count += 1\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(\"null cnt\",count)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_cut 330\n",
    "# char_cut 580\n",
    "text_1_list = list(data['word_cut'])\n",
    "text_3_list = list(data['char_cut'])\n",
    "\n",
    "print('开始序列化')\n",
    "x1, index_1 = set_tokenizer(text_1_list, split_char=' ', max_len=330)\n",
    "x3, index_3 = set_tokenizer(text_3_list, split_char=' ', max_len=580)\n",
    "print('序列化完成')\n",
    "gc.collect()\n",
    "\n",
    "# trian_save_word2vec(text_1_list, save_name='w2v_model/word_w2v_300.txt', split_char=' ')\n",
    "# gc.collect()\n",
    "# trian_save_word2vec(text_3_list, save_name='w2v_model/char_w2v_300.txt', split_char=' ')\n",
    "# gc.collect()\n",
    "# emb1_extend_1 = load_fasttext(index_1, path='w2v_model/cc.zh.300.vec')\n",
    "# emb1_extend_2 = get_embedding_matrix_txt(index_1, embed_size=200, Emed_path=\"w2v_model/Tencent_AILab_ChineseEmbedding.txt\")\n",
    "# np.save('tmp/emb1_extend_1', emb1_extend_1)\n",
    "# np.save('tmp/emb1_extend_2', emb1_extend_2)\n",
    "\n",
    "# 得到emb矩阵\n",
    "emb1 = get_embedding_matrix(index_1, Emed_path='w2v_model/word_w2v_300.txt')\n",
    "emb1_extend_1 = np.load('tmp/emb1_extend_1.npy')\n",
    "emb1_extend_2 = np.load('tmp/emb1_extend_2.npy')\n",
    "emb1 = np.concatenate((emb1, emb1_extend_1, emb1_extend_2), axis=1)\n",
    "\n",
    "emb3 = get_embedding_matrix(index_3, Emed_path='w2v_model/char_w2v_300.txt')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将feature作为输入进行处理(这个feature一定要替换掉nan与inf)\n",
    "# 这个feature的顺序一定要对应之前的feature的顺序，保证每条是对应的\n",
    "feature = data[['word_len', 'char_len']]\n",
    "feature = feature.fillna(-1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss=StandardScaler()\n",
    "ss.fit(feature)\n",
    "hin_feature = ss.transform(feature)\n",
    "num_feature_input = hin_feature.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 区分开train和valid,test\n",
    "# 这里是假设三输入\n",
    "data['label'] = data['label'].fillna(-1)\n",
    "train_data = data[data['label']!=-1]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit(train_data['label'])\n",
    "train_data['label'] = labelencoder.transform(train_data['label'])\n",
    "train_input_1 = x1[:len(train_data)]\n",
    "test_input_1 = x1[len(train_data):]\n",
    "train_input_3 = x3[:len(train_data)]\n",
    "test_input_3 = x3[len(train_data):]\n",
    "train_input_5 = hin_feature[:len(train_data)]\n",
    "test_input_5 = hin_feature[len(train_data):]\n",
    "label = to_categorical(train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import *\n",
    "\n",
    "def model_conv(emb1, emb3, num_feature_input):\n",
    "    '''\n",
    "    注意这个inputs\n",
    "    seq1、seq2分别是两个输入\n",
    "    hin是feature层输入\n",
    "    是否做emb可选可不选，\n",
    "    这个就是我们之前训练已经得到的用于embedding的（embedding_matrix1， embedding_matrix2）\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    emb_layer_1 = Embedding(\n",
    "        input_dim=emb1.shape[0],\n",
    "        output_dim=emb1.shape[1],\n",
    "        weights=[emb1],\n",
    "        input_length=330,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_3 = Embedding(\n",
    "        input_dim=emb3.shape[0],\n",
    "        output_dim=emb3.shape[1],\n",
    "        weights=[emb3],\n",
    "        input_length=580,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    seq1 = Input(shape=(330,))\n",
    "    seq3 = Input(shape=(580,))    \n",
    "    \n",
    "    x1 = emb_layer_1(seq1)\n",
    "    x3 = emb_layer_3(seq3)\n",
    "    \n",
    "    sdrop=SpatialDropout1D(rate=0.2)\n",
    "\n",
    "    x1 = sdrop(x1)\n",
    "    x3 = sdrop(x3)\n",
    "     \n",
    "    x = Dropout(0.2)(Bidirectional(CuDNNGRU(256, return_sequences=True))(x1))\n",
    "    semantic = TimeDistributed(Dense(100, activation=\"tanh\"))(x)\n",
    "    merged_1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(100,))(semantic)\n",
    "    \n",
    "    x = Dropout(0.2)(Bidirectional(CuDNNGRU(256, return_sequences=True))(x3))\n",
    "    semantic = TimeDistributed(Dense(100, activation=\"tanh\"))(x)\n",
    "    merged_3 = Lambda(lambda x: K.max(x, axis=1), output_shape=(100,))(semantic)\n",
    "    \n",
    "    hin = Input(shape=(num_feature_input, ))\n",
    "    htime = Dense(16, activation='relu')(hin)\n",
    "    \n",
    "    x = concatenate([merged_1, merged_3, htime])\n",
    "    \n",
    "    x = Dropout(0.2)(Activation(activation=\"relu\")(BatchNormalization()(Dense(1000)(x))))\n",
    "    x = Activation(activation=\"relu\")(BatchNormalization()(Dense(500)(x)))\n",
    "    pred = Dense(125, activation='softmax')(x)\n",
    "    model = Model(inputs=[seq1, seq3, hin], outputs=pred)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=AdamW(lr=0.001,weight_decay=0.08,),metrics=[\"accuracy\"])\n",
    "    lookahead = Lookahead(k=5, alpha=0.5) # Initialize Lookahead\n",
    "    lookahead.inject(model) # add into model\n",
    "    return model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD |  1\n",
      "#########################################################################################################\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 330)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 580)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 330, 800)     225556000   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 580, 300)     2626200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro multiple             0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 330, 512)     1625088     spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 580, 512)     857088      spatial_dropout1d_1[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 330, 512)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 580, 512)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 330, 100)     51300       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 580, 100)     51300       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 100)          0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 100)          0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           48          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 216)          0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1000)         217000      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1000)         4000        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1000)         0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1000)         0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 500)          500500      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 500)          2000        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 500)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 125)          62625       activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 231,553,149\n",
      "Trainable params: 3,367,949\n",
      "Non-trainable params: 228,185,200\n",
      "__________________________________________________________________________________________________\n",
      "Train on 27969 samples, validate on 3165 samples\n",
      "Epoch 1/50\n",
      "27969/27969 [==============================] - 53s 2ms/step - loss: 2.9186 - acc: 0.3721 - val_loss: 2.1984 - val_acc: 0.4736\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.47362, saving model to model/v1/0.h5\n",
      "Epoch 2/50\n",
      "27969/27969 [==============================] - 51s 2ms/step - loss: 1.8717 - acc: 0.5323 - val_loss: 1.7928 - val_acc: 0.5387\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.47362 to 0.53870, saving model to model/v1/0.h5\n",
      "Epoch 3/50\n",
      "27969/27969 [==============================] - 51s 2ms/step - loss: 1.5974 - acc: 0.5806 - val_loss: 1.6274 - val_acc: 0.5712\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.53870 to 0.57125, saving model to model/v1/0.h5\n",
      "Epoch 4/50\n",
      "27969/27969 [==============================] - 51s 2ms/step - loss: 1.4603 - acc: 0.6046 - val_loss: 1.5572 - val_acc: 0.5845\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.57125 to 0.58452, saving model to model/v1/0.h5\n",
      "Epoch 5/50\n",
      "27969/27969 [==============================] - 51s 2ms/step - loss: 1.3726 - acc: 0.6205 - val_loss: 1.5013 - val_acc: 0.5997\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.58452 to 0.59968, saving model to model/v1/0.h5\n",
      "Epoch 6/50\n",
      "27969/27969 [==============================] - 51s 2ms/step - loss: 1.3031 - acc: 0.6357 - val_loss: 1.5156 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.59968 to 0.60379, saving model to model/v1/0.h5\n",
      "Epoch 7/50\n",
      "27969/27969 [==============================] - 51s 2ms/step - loss: 1.2422 - acc: 0.6508 - val_loss: 1.4806 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.60379\n",
      "Epoch 8/50\n",
      "27969/27969 [==============================] - 51s 2ms/step - loss: 1.1903 - acc: 0.6582 - val_loss: 1.4789 - val_acc: 0.6003\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.60379\n",
      "Epoch 9/50\n",
      "27969/27969 [==============================] - 51s 2ms/step - loss: 1.1470 - acc: 0.6687 - val_loss: 1.4930 - val_acc: 0.5965\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.60379\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 10/50\n",
      "27969/27969 [==============================] - 51s 2ms/step - loss: 1.0696 - acc: 0.6898 - val_loss: 1.4904 - val_acc: 0.6019\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.60379\n",
      "Epoch 11/50\n",
      "27969/27969 [==============================] - 51s 2ms/step - loss: 1.0423 - acc: 0.6966 - val_loss: 1.4886 - val_acc: 0.6006\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.60379\n",
      "Epoch 00011: early stopping\n",
      "3165/3165 [==============================] - 2s 592us/step\n",
      "99999/99999 [==============================] - 49s 488us/step\n",
      "FOLD |  2\n",
      "#########################################################################################################\n",
      "Train on 27976 samples, validate on 3158 samples\n",
      "Epoch 1/50\n",
      "27976/27976 [==============================] - 53s 2ms/step - loss: 2.8752 - acc: 0.3754 - val_loss: 2.2041 - val_acc: 0.4778\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.47783, saving model to model/v1/1.h5\n",
      "Epoch 2/50\n",
      "27976/27976 [==============================] - 51s 2ms/step - loss: 1.8787 - acc: 0.5325 - val_loss: 1.8652 - val_acc: 0.5174\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.47783 to 0.51742, saving model to model/v1/1.h5\n",
      "Epoch 3/50\n",
      "27976/27976 [==============================] - 51s 2ms/step - loss: 1.6006 - acc: 0.5777 - val_loss: 1.6613 - val_acc: 0.5611\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.51742 to 0.56111, saving model to model/v1/1.h5\n",
      "Epoch 4/50\n",
      "27976/27976 [==============================] - 51s 2ms/step - loss: 1.4551 - acc: 0.6081 - val_loss: 1.5988 - val_acc: 0.5776\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.56111 to 0.57758, saving model to model/v1/1.h5\n",
      "Epoch 5/50\n",
      "27976/27976 [==============================] - 51s 2ms/step - loss: 1.3580 - acc: 0.6254 - val_loss: 1.5701 - val_acc: 0.5931\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.57758 to 0.59310, saving model to model/v1/1.h5\n",
      "Epoch 6/50\n",
      "27976/27976 [==============================] - 51s 2ms/step - loss: 1.2874 - acc: 0.6398 - val_loss: 1.5382 - val_acc: 0.5855\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.59310\n",
      "Epoch 7/50\n",
      "27976/27976 [==============================] - 51s 2ms/step - loss: 1.2304 - acc: 0.6519 - val_loss: 1.5376 - val_acc: 0.5934\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.59310 to 0.59341, saving model to model/v1/1.h5\n",
      "Epoch 8/50\n",
      "27976/27976 [==============================] - 51s 2ms/step - loss: 1.1816 - acc: 0.6625 - val_loss: 1.5268 - val_acc: 0.6032\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.59341 to 0.60323, saving model to model/v1/1.h5\n",
      "Epoch 9/50\n",
      "27976/27976 [==============================] - 51s 2ms/step - loss: 1.1334 - acc: 0.6746 - val_loss: 1.5353 - val_acc: 0.5947\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.60323\n",
      "Epoch 10/50\n",
      "27976/27976 [==============================] - 51s 2ms/step - loss: 1.0925 - acc: 0.6814 - val_loss: 1.5311 - val_acc: 0.5950\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.60323\n",
      "Epoch 11/50\n",
      "27976/27976 [==============================] - 51s 2ms/step - loss: 1.0532 - acc: 0.6878 - val_loss: 1.5482 - val_acc: 0.5953\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.60323\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 12/50\n",
      "27976/27976 [==============================] - 51s 2ms/step - loss: 0.9775 - acc: 0.7084 - val_loss: 1.5429 - val_acc: 0.6010\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.60323\n",
      "Epoch 13/50\n",
      "27976/27976 [==============================] - 51s 2ms/step - loss: 0.9499 - acc: 0.7160 - val_loss: 1.5509 - val_acc: 0.6010\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.60323\n",
      "Epoch 00013: early stopping\n",
      "3158/3158 [==============================] - 2s 586us/step\n",
      "99999/99999 [==============================] - 49s 487us/step\n",
      "FOLD |  3\n",
      "#########################################################################################################\n",
      "Train on 27988 samples, validate on 3146 samples\n",
      "Epoch 1/50\n",
      "27988/27988 [==============================] - 53s 2ms/step - loss: 2.8995 - acc: 0.3774 - val_loss: 2.2235 - val_acc: 0.4698\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.46980, saving model to model/v1/2.h5\n",
      "Epoch 2/50\n",
      "27988/27988 [==============================] - 51s 2ms/step - loss: 1.8957 - acc: 0.5273 - val_loss: 1.8019 - val_acc: 0.5404\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.46980 to 0.54037, saving model to model/v1/2.h5\n",
      "Epoch 3/50\n",
      "27988/27988 [==============================] - 51s 2ms/step - loss: 1.6096 - acc: 0.5739 - val_loss: 1.5983 - val_acc: 0.5861\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.54037 to 0.58614, saving model to model/v1/2.h5\n",
      "Epoch 4/50\n",
      "27988/27988 [==============================] - 51s 2ms/step - loss: 1.4583 - acc: 0.6056 - val_loss: 1.5210 - val_acc: 0.5995\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.58614 to 0.59949, saving model to model/v1/2.h5\n",
      "Epoch 5/50\n",
      "27988/27988 [==============================] - 51s 2ms/step - loss: 1.3700 - acc: 0.6200 - val_loss: 1.5011 - val_acc: 0.6062\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.59949 to 0.60617, saving model to model/v1/2.h5\n",
      "Epoch 6/50\n",
      "27988/27988 [==============================] - 51s 2ms/step - loss: 1.3003 - acc: 0.6357 - val_loss: 1.4913 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.60617\n",
      "Epoch 7/50\n",
      "27988/27988 [==============================] - 51s 2ms/step - loss: 1.2413 - acc: 0.6497 - val_loss: 1.4533 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.60617 to 0.60903, saving model to model/v1/2.h5\n",
      "Epoch 8/50\n",
      "27988/27988 [==============================] - 51s 2ms/step - loss: 1.1886 - acc: 0.6613 - val_loss: 1.4734 - val_acc: 0.6020\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.60903\n",
      "Epoch 9/50\n",
      "27988/27988 [==============================] - 51s 2ms/step - loss: 1.1486 - acc: 0.6686 - val_loss: 1.4749 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.60903 to 0.61189, saving model to model/v1/2.h5\n",
      "Epoch 10/50\n",
      "27988/27988 [==============================] - 51s 2ms/step - loss: 1.1016 - acc: 0.6796 - val_loss: 1.4721 - val_acc: 0.6068\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.61189\n",
      "Epoch 11/50\n",
      "27988/27988 [==============================] - 51s 2ms/step - loss: 1.0604 - acc: 0.6859 - val_loss: 1.5067 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.61189\n",
      "Epoch 12/50\n",
      "27988/27988 [==============================] - 51s 2ms/step - loss: 1.0207 - acc: 0.6969 - val_loss: 1.5262 - val_acc: 0.5954\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.61189\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 13/50\n",
      "27988/27988 [==============================] - 51s 2ms/step - loss: 0.9487 - acc: 0.7128 - val_loss: 1.5137 - val_acc: 0.6081\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.61189\n",
      "Epoch 14/50\n",
      "27988/27988 [==============================] - 51s 2ms/step - loss: 0.9118 - acc: 0.7240 - val_loss: 1.5238 - val_acc: 0.5992\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.61189\n",
      "Epoch 00014: early stopping\n",
      "3146/3146 [==============================] - 2s 589us/step\n",
      "99999/99999 [==============================] - 49s 488us/step\n",
      "FOLD |  4\n",
      "#########################################################################################################\n",
      "Train on 28001 samples, validate on 3133 samples\n",
      "Epoch 1/50\n",
      "28001/28001 [==============================] - 53s 2ms/step - loss: 2.8441 - acc: 0.3755 - val_loss: 2.2116 - val_acc: 0.4801\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.48005, saving model to model/v1/3.h5\n",
      "Epoch 2/50\n",
      "28001/28001 [==============================] - 51s 2ms/step - loss: 1.8635 - acc: 0.5345 - val_loss: 1.7783 - val_acc: 0.5493\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.48005 to 0.54931, saving model to model/v1/3.h5\n",
      "Epoch 3/50\n",
      "28001/28001 [==============================] - 51s 2ms/step - loss: 1.5995 - acc: 0.5791 - val_loss: 1.6214 - val_acc: 0.5723\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.54931 to 0.57229, saving model to model/v1/3.h5\n",
      "Epoch 4/50\n",
      "28001/28001 [==============================] - 51s 2ms/step - loss: 1.4601 - acc: 0.6067 - val_loss: 1.5438 - val_acc: 0.5898\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.57229 to 0.58985, saving model to model/v1/3.h5\n",
      "Epoch 5/50\n",
      "28001/28001 [==============================] - 51s 2ms/step - loss: 1.3753 - acc: 0.6221 - val_loss: 1.5088 - val_acc: 0.6029\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.58985 to 0.60294, saving model to model/v1/3.h5\n",
      "Epoch 6/50\n",
      "28001/28001 [==============================] - 51s 2ms/step - loss: 1.2967 - acc: 0.6364 - val_loss: 1.4705 - val_acc: 0.6112\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.60294 to 0.61124, saving model to model/v1/3.h5\n",
      "Epoch 7/50\n",
      "28001/28001 [==============================] - 51s 2ms/step - loss: 1.2429 - acc: 0.6489 - val_loss: 1.4543 - val_acc: 0.6170\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.61124 to 0.61698, saving model to model/v1/3.h5\n",
      "Epoch 8/50\n",
      "28001/28001 [==============================] - 51s 2ms/step - loss: 1.1950 - acc: 0.6609 - val_loss: 1.4708 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.61698\n",
      "Epoch 9/50\n",
      "28001/28001 [==============================] - 51s 2ms/step - loss: 1.1518 - acc: 0.6690 - val_loss: 1.4925 - val_acc: 0.6013\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.61698\n",
      "Epoch 10/50\n",
      "28001/28001 [==============================] - 51s 2ms/step - loss: 1.1072 - acc: 0.6784 - val_loss: 1.4749 - val_acc: 0.6058\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.61698\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 11/50\n",
      "28001/28001 [==============================] - 51s 2ms/step - loss: 1.0337 - acc: 0.6954 - val_loss: 1.4609 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.61698\n",
      "Epoch 12/50\n",
      "28001/28001 [==============================] - 51s 2ms/step - loss: 1.0003 - acc: 0.7052 - val_loss: 1.4665 - val_acc: 0.6096\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.61698\n",
      "Epoch 00012: early stopping\n",
      "3133/3133 [==============================] - 2s 587us/step\n",
      "99999/99999 [==============================] - 49s 488us/step\n",
      "FOLD |  5\n",
      "#########################################################################################################\n",
      "Train on 28014 samples, validate on 3120 samples\n",
      "Epoch 1/50\n",
      "28014/28014 [==============================] - 53s 2ms/step - loss: 2.8628 - acc: 0.3810 - val_loss: 2.1454 - val_acc: 0.4891\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.48910, saving model to model/v1/4.h5\n",
      "Epoch 2/50\n",
      "28014/28014 [==============================] - 51s 2ms/step - loss: 1.8732 - acc: 0.5310 - val_loss: 1.7581 - val_acc: 0.5446\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.48910 to 0.54455, saving model to model/v1/4.h5\n",
      "Epoch 3/50\n",
      "28014/28014 [==============================] - 51s 2ms/step - loss: 1.6002 - acc: 0.5772 - val_loss: 1.5794 - val_acc: 0.5808\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.54455 to 0.58077, saving model to model/v1/4.h5\n",
      "Epoch 4/50\n",
      "28014/28014 [==============================] - 51s 2ms/step - loss: 1.4693 - acc: 0.6023 - val_loss: 1.5251 - val_acc: 0.5881\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.58077 to 0.58814, saving model to model/v1/4.h5\n",
      "Epoch 5/50\n",
      "28014/28014 [==============================] - 51s 2ms/step - loss: 1.3717 - acc: 0.6234 - val_loss: 1.4595 - val_acc: 0.6093\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.58814 to 0.60929, saving model to model/v1/4.h5\n",
      "Epoch 6/50\n",
      "28014/28014 [==============================] - 51s 2ms/step - loss: 1.3032 - acc: 0.6348 - val_loss: 1.4612 - val_acc: 0.6064\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.60929\n",
      "Epoch 7/50\n",
      "28014/28014 [==============================] - 51s 2ms/step - loss: 1.2461 - acc: 0.6497 - val_loss: 1.4492 - val_acc: 0.6087\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.60929\n",
      "Epoch 8/50\n",
      "28014/28014 [==============================] - 51s 2ms/step - loss: 1.1984 - acc: 0.6577 - val_loss: 1.4505 - val_acc: 0.6071\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.60929\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 9/50\n",
      "28014/28014 [==============================] - 51s 2ms/step - loss: 1.1256 - acc: 0.6756 - val_loss: 1.4141 - val_acc: 0.6205\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.60929 to 0.62051, saving model to model/v1/4.h5\n",
      "Epoch 10/50\n",
      "28014/28014 [==============================] - 51s 2ms/step - loss: 1.0925 - acc: 0.6832 - val_loss: 1.4226 - val_acc: 0.6170\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.62051\n",
      "Epoch 11/50\n",
      "28014/28014 [==============================] - 51s 2ms/step - loss: 1.0619 - acc: 0.6906 - val_loss: 1.4275 - val_acc: 0.6144\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.62051\n",
      "Epoch 12/50\n",
      "28014/28014 [==============================] - 51s 2ms/step - loss: 1.0352 - acc: 0.6947 - val_loss: 1.4274 - val_acc: 0.6160\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.62051\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 13/50\n",
      "28014/28014 [==============================] - 51s 2ms/step - loss: 0.9965 - acc: 0.7055 - val_loss: 1.4290 - val_acc: 0.6128\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.62051\n",
      "Epoch 14/50\n",
      "28014/28014 [==============================] - 51s 2ms/step - loss: 0.9777 - acc: 0.7113 - val_loss: 1.4335 - val_acc: 0.6135\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.62051\n",
      "Epoch 00014: early stopping\n",
      "3120/3120 [==============================] - 2s 587us/step\n",
      "99999/99999 [==============================] - 49s 488us/step\n",
      "FOLD |  6\n",
      "#########################################################################################################\n",
      "Train on 28023 samples, validate on 3111 samples\n",
      "Epoch 1/50\n",
      "28023/28023 [==============================] - 53s 2ms/step - loss: 2.9309 - acc: 0.3690 - val_loss: 2.2053 - val_acc: 0.4699\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.46995, saving model to model/v1/5.h5\n",
      "Epoch 2/50\n",
      "28023/28023 [==============================] - 51s 2ms/step - loss: 1.9044 - acc: 0.5266 - val_loss: 1.8043 - val_acc: 0.5368\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.46995 to 0.53680, saving model to model/v1/5.h5\n",
      "Epoch 3/50\n",
      "28023/28023 [==============================] - 51s 2ms/step - loss: 1.6123 - acc: 0.5753 - val_loss: 1.6497 - val_acc: 0.5773\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.53680 to 0.57731, saving model to model/v1/5.h5\n",
      "Epoch 4/50\n",
      "28023/28023 [==============================] - 51s 2ms/step - loss: 1.4641 - acc: 0.6049 - val_loss: 1.5692 - val_acc: 0.5940\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.57731 to 0.59402, saving model to model/v1/5.h5\n",
      "Epoch 5/50\n",
      "28023/28023 [==============================] - 51s 2ms/step - loss: 1.3721 - acc: 0.6228 - val_loss: 1.5250 - val_acc: 0.6043\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.59402 to 0.60431, saving model to model/v1/5.h5\n",
      "Epoch 6/50\n",
      "28023/28023 [==============================] - 51s 2ms/step - loss: 1.3014 - acc: 0.6392 - val_loss: 1.5144 - val_acc: 0.6030\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.60431\n",
      "Epoch 7/50\n",
      "28023/28023 [==============================] - 51s 2ms/step - loss: 1.2462 - acc: 0.6514 - val_loss: 1.5123 - val_acc: 0.6024\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.60431\n",
      "Epoch 8/50\n",
      "28023/28023 [==============================] - 51s 2ms/step - loss: 1.1955 - acc: 0.6581 - val_loss: 1.4914 - val_acc: 0.6062\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.60431 to 0.60624, saving model to model/v1/5.h5\n",
      "Epoch 9/50\n",
      "28023/28023 [==============================] - 51s 2ms/step - loss: 1.1456 - acc: 0.6695 - val_loss: 1.4964 - val_acc: 0.6033\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.60624\n",
      "Epoch 10/50\n",
      "28023/28023 [==============================] - 51s 2ms/step - loss: 1.1024 - acc: 0.6787 - val_loss: 1.4990 - val_acc: 0.6078\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.60624 to 0.60784, saving model to model/v1/5.h5\n",
      "Epoch 11/50\n",
      "28023/28023 [==============================] - 51s 2ms/step - loss: 1.0579 - acc: 0.6903 - val_loss: 1.5011 - val_acc: 0.6033\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.60784\n",
      "Epoch 12/50\n",
      "28023/28023 [==============================] - 51s 2ms/step - loss: 1.0225 - acc: 0.6972 - val_loss: 1.5025 - val_acc: 0.6056\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.60784\n",
      "Epoch 13/50\n",
      "28023/28023 [==============================] - 51s 2ms/step - loss: 0.9802 - acc: 0.7077 - val_loss: 1.5337 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.60784\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 14/50\n",
      "28023/28023 [==============================] - 51s 2ms/step - loss: 0.9077 - acc: 0.7265 - val_loss: 1.5247 - val_acc: 0.6050\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.60784\n",
      "Epoch 15/50\n",
      "28023/28023 [==============================] - 51s 2ms/step - loss: 0.8810 - acc: 0.7333 - val_loss: 1.5522 - val_acc: 0.5976\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.60784\n",
      "Epoch 00015: early stopping\n",
      "3111/3111 [==============================] - 2s 586us/step\n",
      "99999/99999 [==============================] - 49s 488us/step\n",
      "FOLD |  7\n",
      "#########################################################################################################\n",
      "Train on 28039 samples, validate on 3095 samples\n",
      "Epoch 1/50\n",
      "28039/28039 [==============================] - 53s 2ms/step - loss: 2.8968 - acc: 0.3736 - val_loss: 2.1378 - val_acc: 0.4953\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.49532, saving model to model/v1/6.h5\n",
      "Epoch 2/50\n",
      "28039/28039 [==============================] - 51s 2ms/step - loss: 1.8841 - acc: 0.5287 - val_loss: 1.7662 - val_acc: 0.5444\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.49532 to 0.54443, saving model to model/v1/6.h5\n",
      "Epoch 3/50\n",
      "28039/28039 [==============================] - 51s 2ms/step - loss: 1.6064 - acc: 0.5805 - val_loss: 1.6002 - val_acc: 0.5754\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.54443 to 0.57544, saving model to model/v1/6.h5\n",
      "Epoch 4/50\n",
      "28039/28039 [==============================] - 51s 2ms/step - loss: 1.4643 - acc: 0.6054 - val_loss: 1.5336 - val_acc: 0.5887\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.57544 to 0.58869, saving model to model/v1/6.h5\n",
      "Epoch 5/50\n",
      "28039/28039 [==============================] - 51s 2ms/step - loss: 1.3702 - acc: 0.6220 - val_loss: 1.4931 - val_acc: 0.6071\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.58869 to 0.60711, saving model to model/v1/6.h5\n",
      "Epoch 6/50\n",
      "28039/28039 [==============================] - 51s 2ms/step - loss: 1.3024 - acc: 0.6360 - val_loss: 1.4505 - val_acc: 0.6200\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.60711 to 0.62003, saving model to model/v1/6.h5\n",
      "Epoch 7/50\n",
      "28039/28039 [==============================] - 51s 2ms/step - loss: 1.2452 - acc: 0.6510 - val_loss: 1.4476 - val_acc: 0.6142\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.62003\n",
      "Epoch 8/50\n",
      "28039/28039 [==============================] - 51s 2ms/step - loss: 1.1859 - acc: 0.6592 - val_loss: 1.4560 - val_acc: 0.6142\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.62003\n",
      "Epoch 9/50\n",
      "28039/28039 [==============================] - 51s 2ms/step - loss: 1.1451 - acc: 0.6692 - val_loss: 1.4710 - val_acc: 0.6149\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.62003\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 10/50\n",
      "28039/28039 [==============================] - 51s 2ms/step - loss: 1.0697 - acc: 0.6870 - val_loss: 1.4328 - val_acc: 0.6236\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.62003 to 0.62359, saving model to model/v1/6.h5\n",
      "Epoch 11/50\n",
      "28039/28039 [==============================] - 51s 2ms/step - loss: 1.0389 - acc: 0.6950 - val_loss: 1.4524 - val_acc: 0.6187\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.62359\n",
      "Epoch 12/50\n",
      "28039/28039 [==============================] - 51s 2ms/step - loss: 1.0163 - acc: 0.6993 - val_loss: 1.4579 - val_acc: 0.6162\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.62359\n",
      "Epoch 13/50\n",
      "28039/28039 [==============================] - 51s 2ms/step - loss: 0.9865 - acc: 0.7053 - val_loss: 1.4680 - val_acc: 0.6139\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.62359\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 14/50\n",
      "28039/28039 [==============================] - 51s 2ms/step - loss: 0.9435 - acc: 0.7191 - val_loss: 1.4738 - val_acc: 0.6158\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.62359\n",
      "Epoch 15/50\n",
      "28039/28039 [==============================] - 51s 2ms/step - loss: 0.9317 - acc: 0.7180 - val_loss: 1.4780 - val_acc: 0.6197\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.62359\n",
      "Epoch 00015: early stopping\n",
      "3095/3095 [==============================] - 2s 588us/step\n",
      "99999/99999 [==============================] - 49s 489us/step\n",
      "FOLD |  8\n",
      "#########################################################################################################\n",
      "Train on 28053 samples, validate on 3081 samples\n",
      "Epoch 1/50\n",
      "28053/28053 [==============================] - 53s 2ms/step - loss: 2.8693 - acc: 0.3718 - val_loss: 2.1373 - val_acc: 0.4898\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.48978, saving model to model/v1/7.h5\n",
      "Epoch 2/50\n",
      "28053/28053 [==============================] - 51s 2ms/step - loss: 1.8804 - acc: 0.5261 - val_loss: 1.7758 - val_acc: 0.5411\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.48978 to 0.54106, saving model to model/v1/7.h5\n",
      "Epoch 3/50\n",
      "28053/28053 [==============================] - 51s 2ms/step - loss: 1.6084 - acc: 0.5772 - val_loss: 1.6094 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.54106 to 0.57871, saving model to model/v1/7.h5\n",
      "Epoch 4/50\n",
      "28053/28053 [==============================] - 51s 2ms/step - loss: 1.4654 - acc: 0.6051 - val_loss: 1.5294 - val_acc: 0.5884\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.57871 to 0.58845, saving model to model/v1/7.h5\n",
      "Epoch 5/50\n",
      "28053/28053 [==============================] - 51s 2ms/step - loss: 1.3723 - acc: 0.6221 - val_loss: 1.4537 - val_acc: 0.6027\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.58845 to 0.60273, saving model to model/v1/7.h5\n",
      "Epoch 6/50\n",
      "28053/28053 [==============================] - 51s 2ms/step - loss: 1.3061 - acc: 0.6380 - val_loss: 1.4513 - val_acc: 0.5988\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.60273\n",
      "Epoch 7/50\n",
      "28053/28053 [==============================] - 51s 2ms/step - loss: 1.2503 - acc: 0.6479 - val_loss: 1.4177 - val_acc: 0.6099\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.60273 to 0.60987, saving model to model/v1/7.h5\n",
      "Epoch 8/50\n",
      "28053/28053 [==============================] - 51s 2ms/step - loss: 1.1960 - acc: 0.6615 - val_loss: 1.4401 - val_acc: 0.6014\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.60987\n",
      "Epoch 9/50\n",
      "28053/28053 [==============================] - 51s 2ms/step - loss: 1.1513 - acc: 0.6682 - val_loss: 1.4200 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.60987 to 0.61733, saving model to model/v1/7.h5\n",
      "Epoch 10/50\n",
      "28053/28053 [==============================] - 51s 2ms/step - loss: 1.1066 - acc: 0.6792 - val_loss: 1.4571 - val_acc: 0.5992\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.61733\n",
      "Epoch 11/50\n",
      "28053/28053 [==============================] - 51s 2ms/step - loss: 1.0645 - acc: 0.6891 - val_loss: 1.4505 - val_acc: 0.6066\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.61733\n",
      "Epoch 12/50\n",
      "26624/28053 [===========================>..] - ETA: 2s - loss: 1.0253 - acc: 0.6955"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10, random_state=1017, shuffle=True)\n",
    "sub = np.zeros((test_input_5.shape[0], 125))\n",
    "oof_pred = np.zeros((train_input_5.shape[0], 125))\n",
    "score = []\n",
    "count = 0\n",
    "version = 'v1'\n",
    "if not os.path.exists(\"model/\"+ version + \"/\"):\n",
    "    os.mkdir(\"model/\"+ version + \"/\")\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(train_input_5, train_data['label'])):\n",
    "    print(\"FOLD | \", count+1)\n",
    "    print(\"###\"*35)\n",
    "    gc.collect()\n",
    "    filepath = \"model/\"+ version + \"/%d.h5\" % count\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_acc', factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n",
    "    earlystopping = EarlyStopping(\n",
    "        monitor='val_acc', min_delta=0.0001, patience=5, verbose=1, mode='max')\n",
    "    callbacks = [checkpoint, reduce_lr, earlystopping]\n",
    "    model_age = model_conv(emb1, emb3, num_feature_input)\n",
    "    if count==0:model_age.summary()\n",
    "    x1_tr, x1_va = np.array(train_input_1)[train_index], np.array(train_input_1)[test_index]    \n",
    "    x3_tr, x3_va = np.array(train_input_3)[train_index], np.array(train_input_3)[test_index]\n",
    "    x5_tr, x5_va = np.array(train_input_5)[train_index], np.array(train_input_5)[test_index]\n",
    "    y_tr, y_va = label[train_index], label[test_index]\n",
    "    \n",
    "    hist = model_age.fit([x1_tr, x3_tr, x5_tr],\n",
    "                         y_tr, batch_size=512, epochs=50, \n",
    "                         validation_data=([x1_va, x3_va, x5_va], y_va),\n",
    "                         callbacks=callbacks, verbose=1, shuffle=True)\n",
    "\n",
    "    model_age.load_weights(filepath)\n",
    "    oof_pred[test_index] = model_age.predict([x1_va, x3_va, x5_va],batch_size=1024,verbose=1)\n",
    "    sub += model_age.predict([test_input_1, test_input_3, test_input_5],batch_size=1024,verbose=1)/skf.n_splits\n",
    "    score.append(np.max(hist.history['val_acc']))\n",
    "    count += 1\n",
    "print('acc:', np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7629279886940322"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred = np.argsort(-oof_pred, axis=1)\n",
    "new_pred = new_pred[:,:2]\n",
    "in_it = 0\n",
    "for i, value in enumerate(list(train_data['label'])):\n",
    "    if value in list(new_pred[i, :2]):\n",
    "        in_it += 1\n",
    "in_it/len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "put_result = np.argsort(-sub, axis=1)\n",
    "label_1 = put_result[:,0]\n",
    "label_2 = put_result[:,1]\n",
    "data['label'] = data['label'].fillna(-1)\n",
    "test_data = data[data['label']==-1]\n",
    "result = pd.DataFrame()\n",
    "result['id'] = list(test_data['id'])\n",
    "result['label1'] = list(labelencoder.inverse_transform(label_1))\n",
    "result['label2'] = list(labelencoder.inverse_transform(label_2))\n",
    "result.to_csv('result/v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.DataFrame(oof_pred)\n",
    "te = pd.DataFrame(sub)\n",
    "stack_result = pd.concat([tr, te], axis=0, sort=False)\n",
    "new_columns = []\n",
    "for i in stack_result.columns:\n",
    "    new_columns.append(int(i))\n",
    "new_columns = labelencoder.inverse_transform(new_columns)\n",
    "stack_result.columns = new_columns\n",
    "stack_result['id'] = data['id']\n",
    "stack_result.to_csv('result/stack_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
