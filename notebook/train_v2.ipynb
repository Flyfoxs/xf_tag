{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "887cc6be0765929d5e382a830efdb902bd9ce99b"
   },
   "source": [
    "# Baidu Emotion.\n",
    "\n",
    "### Let's start exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-02 20:14:52,472 util_log.py[61] DEBUG Start the program at:LALI2-M-G0MD, 127.0.0.1, with:Load module\n"
     ]
    }
   ],
   "source": [
    "#Adjust the working folder\n",
    "import sys\n",
    "import os\n",
    "#print(globals())\n",
    "file_folder = globals()['_dh'][0]\n",
    "wk_dir = os.path.dirname(file_folder)\n",
    "os.chdir(wk_dir)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from code_felix.core.config import *\n",
    "from code_felix.core.feature import *\n",
    "from file_cache.utils.util_log import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "#Loading the dataset\n",
    "dataset = pd.read_csv(train_file, encoding='gb18030', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>食品餐饮</td>\n",
       "      <td>买这套系统本来是用来做我们公司的公众号第三方平台代运营的，没想到还有app，而且每个都很方便...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>食品餐饮</td>\n",
       "      <td>烤鸭还是不错的，别的菜没什么特殊的</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>食品餐饮</td>\n",
       "      <td>使用说明看不懂！不会用，很多操作没详细标明！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1                                                  2  3\n",
       "0  1  食品餐饮  买这套系统本来是用来做我们公司的公众号第三方平台代运营的，没想到还有app，而且每个都很方便...  2\n",
       "1  2  食品餐饮                                  烤鸭还是不错的，别的菜没什么特殊的  1\n",
       "2  3  食品餐饮                             使用说明看不懂！不会用，很多操作没详细标明！  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prin some samples\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b2e3ea8267e3bac72a30ce7803413c684bc8b9a4"
   },
   "source": [
    "## Preparing data for model training\n",
    "### Tokenization\n",
    "Since the data is already tokenized and lowercased, we just need to split the words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "cb739f05cfb4b5d74702cdef1ea5a130c0d90132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2019-01-02 20:14:52,758 __init__.py[111] DEBUG Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/d2/vq91lnt11m13m84s18dzdm8r0000gn/T/jieba.cache\n",
      "2019-01-02 20:14:52,766 __init__.py[131] DEBUG Loading model from cache /var/folders/d2/vq91lnt11m13m84s18dzdm8r0000gn/T/jieba.cache\n",
      "Loading model cost 0.715 seconds.\n",
      "2019-01-02 20:14:53,479 __init__.py[163] DEBUG Loading model cost 0.715 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-01-02 20:14:53,483 __init__.py[164] DEBUG Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "input_sentences = [list(jieba.cut(str(text), cut_all=False))  for text in dataset.iloc[:, 2].values.tolist()]\n",
    "labels = dataset.iloc[:, 3].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 0, 0, 1, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a7c2e03d7e839b2872785157153e0bfef82b0bd"
   },
   "source": [
    "### Creating Vocabulary (word index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-02 20:16:43,159 util_log.py[41] INFO get_word_id_vec begin with(1 paras) :['02'], []\n",
      "2019-01-02 20:16:43,175 cache.py[29] DEBUG try to read cache from file:./cache/get_word_id_vec=02=.h5, (h5, key:['/df_0'])\n",
      "2019-01-02 20:16:43,221 util_log.py[49] INFO get_word_id_vec cost    0.06 sec:(1 paras)(['02'], []), return:DataFrame, end \n",
      "2019-01-02 20:16:47,027 <ipython-input-14-102863bb139b>[3] DEBUG Word length:42014\n"
     ]
    }
   ],
   "source": [
    "word_id_vec =  get_word_id_vec('02')\n",
    "word2id = dict( word_id_vec.apply(lambda row: (row['word'], row['id']), axis=1).values )\n",
    "logger.debug(f'Word length:{len(word2id)}')\n",
    "\n",
    "\n",
    "embedding_weights = word_id_vec.iloc[:, -vector_size:].fillna(0).values\n",
    "#embedding_weights[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "f60be75ae0d5cbfc36eeba0243407b66741bb42e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-02 20:14:58,137 <ipython-input-7-ae333458653d>[18] DEBUG max_words=38\n",
      "2019-01-02 20:14:58,141 <ipython-input-7-ae333458653d>[18] DEBUG max_words=73\n",
      "2019-01-02 20:14:58,143 <ipython-input-7-ae333458653d>[18] DEBUG max_words=77\n",
      "2019-01-02 20:14:58,146 <ipython-input-7-ae333458653d>[18] DEBUG max_words=88\n",
      "2019-01-02 20:14:58,148 <ipython-input-7-ae333458653d>[18] DEBUG max_words=93\n",
      "2019-01-02 20:14:58,150 <ipython-input-7-ae333458653d>[18] DEBUG max_words=110\n",
      "2019-01-02 20:14:58,152 <ipython-input-7-ae333458653d>[18] DEBUG max_words=151\n",
      "2019-01-02 20:14:58,154 <ipython-input-7-ae333458653d>[18] DEBUG max_words=171\n",
      "2019-01-02 20:14:58,156 <ipython-input-7-ae333458653d>[18] DEBUG max_words=177\n",
      "2019-01-02 20:14:58,158 <ipython-input-7-ae333458653d>[18] DEBUG max_words=196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize word2id and label2id dictionaries that will be used to encode words and labels\n",
    "\n",
    "label2id = dict()\n",
    "\n",
    "max_words = 0 # maximum number of words in a sentence\n",
    "\n",
    "# Construction of word2id dict\n",
    "for sentence in input_sentences:\n",
    "#     for word in sentence:\n",
    "#         # Add words to word2id dict if not exist\n",
    "#         if word not in word2id:\n",
    "#             word2id[word] = len(word2id)\n",
    "#     # If length of the sentence is greater than max_words, update max_words\n",
    "#     sentence = list(sentence)\n",
    "#     logger.debug(f'{len(sentence)} : {sentence}')\n",
    "    if len(sentence) > max_words:\n",
    "        max_words = len(sentence)\n",
    "        logger.debug(f'max_words={max_words}')\n",
    "    \n",
    "# Construction of label2id and id2label dicts\n",
    "label2id = {l: i for i, l in enumerate(set(labels))}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d984e58ffd25530ac4c05ce623d9237a35cf903d"
   },
   "source": [
    "### Encoding samples with corresponing integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "378ef884a6ebb19b02a70082bc6c854c51780af3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lali2/dev/python/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/lali2/dev/python/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (2000, 196)\n",
      "Shape of Y: (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# Encode input words and labels\n",
    "X = [[word2id[word] for word in sentence] for sentence in input_sentences]\n",
    "Y = [label2id[label] for label in labels]\n",
    "\n",
    "# Apply Padding to X\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X, max_words)\n",
    "\n",
    "# Convert Y to numpy array\n",
    "Y = keras.utils.to_categorical(Y, num_classes=len(label2id))\n",
    "\n",
    "# Print shapes\n",
    "print(\"Shape of X: {}\".format(X.shape))\n",
    "print(\"Shape of Y: {}\".format(Y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bccaa5b813414ad7929522d4d0f74dbb9c4c5af"
   },
   "source": [
    "## Build LSTM model with attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.layers.Embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "4c1b5fc7613a0fe5a8067135e2de07e0765f1b78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 196)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 196, 200)     8402800     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 196, 200)     0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 196, 200)     240800      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 196, 200)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 196, 1)       201         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 196)          0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Activation)      (None, 196)          0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 200)          0           dropout_4[0][0]                  \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 100)          20100       dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 3)            303         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,664,204\n",
      "Trainable params: 8,664,204\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100 # The dimension of word embeddings\n",
    "\n",
    "# Define input tensor\n",
    "sequence_input = keras.Input(shape=(max_words,), dtype='int32')\n",
    "\n",
    "# Word embedding layer\n",
    "embedded_inputs =keras.layers.Embedding(len(word2id) ,\n",
    "                                        vector_size ,\n",
    "                                        input_length=max_words ,\n",
    "                                        weights = [embedding_weights] ,\n",
    "                                       )(sequence_input)\n",
    "\n",
    "# Apply dropout to prevent overfitting\n",
    "embedded_inputs = keras.layers.Dropout(0.2)(embedded_inputs)\n",
    "\n",
    "# Apply Bidirectional LSTM over embedded inputs\n",
    "lstm_outs = keras.layers.wrappers.Bidirectional(\n",
    "    keras.layers.LSTM(embedding_dim, return_sequences=True)\n",
    ")(embedded_inputs)\n",
    "\n",
    "# Apply dropout to LSTM outputs to prevent overfitting\n",
    "lstm_outs = keras.layers.Dropout(0.2)(lstm_outs)\n",
    "\n",
    "# Attention Mechanism - Generate attention vectors\n",
    "input_dim = int(lstm_outs.shape[2])\n",
    "permuted_inputs = keras.layers.Permute((2, 1))(lstm_outs)\n",
    "attention_vector = keras.layers.TimeDistributed(keras.layers.Dense(1))(lstm_outs)\n",
    "attention_vector = keras.layers.Reshape((max_words,))(attention_vector)\n",
    "attention_vector = keras.layers.Activation('softmax', name='attention_vec')(attention_vector)\n",
    "attention_output = keras.layers.Dot(axes=1)([lstm_outs, attention_vector])\n",
    "\n",
    "# Last layer: fully connected with softmax activation\n",
    "fc = keras.layers.Dense(embedding_dim, activation='relu')(attention_output)\n",
    "output = keras.layers.Dense(len(label2id), activation='softmax')(fc)\n",
    "\n",
    "# Finally building model\n",
    "model = keras.Model(inputs=[sequence_input], outputs=output)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer='adam')\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad67135dcd65940d864521309066ff9fb5b7c9a2"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "d9441f027a63ad3c8b288c6823e073b142c33b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/2\n",
      "1800/1800 [==============================] - 18s 10ms/step - loss: 0.8834 - acc: 0.6761 - val_loss: 0.8265 - val_acc: 0.6950\n",
      "Epoch 2/2\n",
      "1800/1800 [==============================] - 16s 9ms/step - loss: 0.8213 - acc: 0.6778 - val_loss: 0.6513 - val_acc: 0.7150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b236a58>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model 10 iterations\n",
    "model.fit(X, Y, epochs=2, batch_size=64, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b37aca89d92439a9777bb7634dcd12aef2162771"
   },
   "source": [
    "The accuracy on validation data about 93%. Very good result for a classification task with six-classes.\n",
    "The performance can be further improved by training the model a few more iteration.\n",
    "\n",
    "**Let's look closer to model predictions and attentions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a5e94835a8aa88b8609a95e80add37fc1ffd4d7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-create the model to get attention vectors as well as label prediction\n",
    "model_with_attentions = keras.Model(inputs=model.input,\n",
    "                                    outputs=[model.output, \n",
    "                                             model.get_layer('attention_vec').output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7f1a8770b09a221787e38376392ba977172c215",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Select random samples to illustrate\n",
    "sample_text = random.choice(dataset[\"text\"].values.tolist())\n",
    "\n",
    "# Encode samples\n",
    "tokenized_sample = sample_text.split(\" \")\n",
    "encoded_samples = [[word2id[word] for word in tokenized_sample]]\n",
    "\n",
    "# Padding\n",
    "encoded_samples = keras.preprocessing.sequence.pad_sequences(encoded_samples, maxlen=max_words)\n",
    "\n",
    "# Make predictions\n",
    "label_probs, attentions = model_with_attentions.predict(encoded_samples)\n",
    "label_probs = {id2label[_id]: prob for (label, _id), prob in zip(label2id.items(),label_probs[0])}\n",
    "\n",
    "# Get word attentions using attenion vector\n",
    "token_attention_dic = {}\n",
    "max_score = 0.0\n",
    "min_score = 0.0\n",
    "for token, attention_score in zip(tokenized_sample, attentions[0][-len(tokenized_sample):]):\n",
    "    token_attention_dic[token] = math.sqrt(attention_score)\n",
    "\n",
    "\n",
    "# VISUALIZATION\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def rgb_to_hex(rgb):\n",
    "    return '#%02x%02x%02x' % rgb\n",
    "    \n",
    "def attention2color(attention_score):\n",
    "    r = 255 - int(attention_score * 255)\n",
    "    color = rgb_to_hex((255, r, r))\n",
    "    return str(color)\n",
    "    \n",
    "# Build HTML String to viualize attentions\n",
    "html_text = \"<hr><p style='font-size: large'><b>Text:  </b>\"\n",
    "for token, attention in token_attention_dic.items():\n",
    "    html_text += \"<span style='background-color:{};'>{} <span> \".format(attention2color(attention),\n",
    "                                                                        token)\n",
    "html_text += \"</p>\"\n",
    "# Display text enriched with attention scores \n",
    "display(HTML(html_text))\n",
    "\n",
    "# PLOT EMOTION SCORES\n",
    "emotions = [label for label, _ in label_probs.items()]\n",
    "scores = [score for _, score in label_probs.items()]\n",
    "plt.figure(figsize=(5,2))\n",
    "plt.bar(np.arange(len(emotions)), scores, align='center', alpha=0.5, color=['black', 'red', 'green', 'blue', 'cyan', \"purple\"])\n",
    "plt.xticks(np.arange(len(emotions)), emotions)\n",
    "plt.ylabel('Scores')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dd58f4f2b92b103765af428baee13a53d80eb4e9"
   },
   "source": [
    "**We have used an attention mechanism with an LSTM network to recognize emotions in given text.\n",
    "We show that attention mechanism can be useful for classification tasks as well as sequence labeling tasks.\n",
    "We have illustrated the attentions in order to make model predictions interpretable and look fancy.\n",
    "Enjoy attentions mechanism in different applications...**\n",
    "\n",
    "*All feedbacks are welcome.*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
